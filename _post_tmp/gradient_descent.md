

## 시작하며



Deep learning을 공부하면서 별 고민 없이 당연히 받아들이는 개념들이 참 많습니다. 지난 포스트에서 살펴본 convolution kernel에 대한 내용이 그랬습니다. 당연히 convolution은 kernel을 한칸씩 혹은 몇칸씩 움직이면서 convolution 연산을 차례대로 해나가는 것이라고 막연히 생각했지만, convolution의 개념을 확장해 가는 과정 (예를 들면 deconvolution, dilated convolution 등)에서 점점더 기본적인 convolution의 정의를 다시 되짚어야 했던 경험이 있습니다. 결국 한발짝 나서기 위해서는 기본적인 정의에 대한 이해가 가장 중요한 것이죠.

지난 주말에 다시 공부하다가 graident descent를 살펴보면서 문득 왜 Gradient vector는 목적함수를 가장 가파르게 증가시키는 방향을 의미하는지가 궁금했습니다. 사실, 이렇게 알려진 것은 하나의 정리로, 과연 어떤 reasoning을 통해 gradient 방향이 왜 그러한 방향인지에 대해서 생각해보다가 처음부터 끝까지 정리된 자료가 그렇게 많지는 않다는 것을 깨달았습니다. 자세히 생각해 보면, 함수의 input이 있고, 함수를 vector 각각의 원소에 대해 편미분을 한 후에 원래의 point에서 적당히(learning rate를 적용합니다.) 빼주면 loss function을 작게 하는 방향으로 input이 변화한다는 게 별로 당연해 보이지는 않습니다. 이에 대해서 좀더 자세하게 알아보고 싶습니다.



## 적절한 setup

기본적으로 $p$개의 feature가 있고, 그 feature의 함수가 우리가 최적화하고자 하는 목적함수(Object function)입니다. 함수의 결과물도 하나의 실수로, 결국 우리는 $p+1$ 차원의 vector 공간을 상정합니다. 
$ y = L(x_1, \ldots, x_p), \textrm{ , where } \{x_1, \ldots, x_p, y \} \in \mathbb{R^{p+1}}$

수학에서는 hyperplane이라는 개념이 있습니다. Wiki에는 다음과 같이 정의되어 있습니다.

> In geometry, a hyperplane is a subspace whose dimension is one less than that of its ambient space.

단지 전체 차원에서 1차원 작은 subspace를 hyperplane입니다. subspace는 그 space에 속한 원소들에 연산을 적용했을 때 그 연산의 결과가 꼭 그 안에 있어야 하는 조건을 가지고 있습니다. 다시 말하면, supspace도 space의 조건을 갖추어야 한다는 것입니다. 3차원 공간에서의 hyperplane은 2차원 공간입니다. 2차원 평면에서는 line이 hyperplane이 됩니다. subspace가 되기 위해서는 꼭 원점을 포함해야 합니다. 

## 벡터 공간 2차원 공간부터...

벡터 공간에 대한 이해가 먼저 있어야 할 것입니다. 수학에서 공간(space)이라고 하는 것은 어떤 원소의 모임(집합)으로서 거기에 기본적인 연산이나 구조가 정의된 것을 의미합니다. 막연히 공간이라는 일반화된 정의는 너무 추상적이고 어렵습니다. 사실 데이터를 분석하면서 우리는 완전히 추상화된 공간을 접할 경우는 거의 없습니다. 제가 경험한 바로는 2017년에 소개된 Poincare embedding 정도가 거의 유일하게 Euclidean 공간을 벗어난 공간 (Euclidean 공간의 평행성 공리가 깨진 공간)이며, 우리는 Euclidean 공간에 내적이라는 개념을 도입함으로써, 공간 상의 두 원소간의 거리와 각도를 정의해 놓은 내적공간(inner product space)만 자세히 알면 될 것 같습니다. 이것이 선형대수학의 연구 대상이구요. 다시 말하면, 선형대수에 대해서만 잘 알고 있으면 데이터 분석에서는 거의 전혀 문제 될 것이 없습니다. 참고적으로는 Euclidean 벡터 공간에서 $n$이 무한대가 되면, Hilbert 공간으로 발전해 나가는데, 이는 함수해석학의 연구 대상입니다. 이렇게 되면 함수가 Hilbert space의 한점으로 표현할 수 있기 때문입니다. 고급 통계 이론들을 이해하고자 한다면 Hilbert space에 대한 이해도 어느정도는 필요하기는 합니다.

먼저 Euclidean 공간을 정의해 보면 다음과 같습니다.

> 음이 아닌 정수 $n = 0, 1, 2, \ldots$에 대해서 $n$ 차원 Euclidean 공간은 $\mathbb R^n$이라고 쓰며, 이는 실수 집합 $\mathbb R$을 $n$번 곱해놓은 집합이다. (Wikipedia)

곱해놓았다는 표시를 하기 위해 우리는 다음과 같은 벡터로 Euclidean 공간의 원소를 나타냅니다. $\mathbb R^0$이라는 공간은 원점만을 포함한 공간을 의미하겠죠. $n$ 차원 공간의 원소는 다음과 같이 $n$개의 실수를 쌓아서 표현합니다.

$$ x = \left(\begin{array}{c}x_1 \\ x_2 \\ \vdots \\ x_n\end{array}\right), \{x_i\}_{i=1}^n \in \mathbb R$$

만약 $n$이 2면 평면을 의미합니다. 평면에서는 (상, 하), (좌, 우) 이렇게 2개의 방향만 존재하기 때문에 숫자 2개로 평면 상의 어떠한 점도 표시가 가능합니다. 우리가 생활하는 3차원 공간은 (상, 하) (좌, 우) (위, 아래) 이렇게 3개의 방향이면 그 어떠한 위치도 표현할 수 있습니다. 이 경우 3개의 숫자를 쌓아서 공간 안의 점을 표현합니다.

Euclidean 공간에 내적이 정의되어 원소간의 거리와 각도가 정의되어 있는 내적공간(inner product space)를 효과적으로 나타내기 위해서 직교 좌표계를 사용합니다. 2차원 공간의 직교 좌표계는 원점에서 만나는 수직선과 수평선으로 이루어집니다.
![2d-coord](/assets/2d-coord.png)

3차원의 직교 좌표계는 원점에서 만나는 서로 직교하는 세개의 평면으로 이루어집니다.

![3d-coordinate](/assets/3d-coordinate.png)



> 공간의 점을 표현함에 있어서 직교 좌표계가 아닌 것 중에 가장 유명한 것을 고르라면 극좌표계가 되겠지요. 공간상의 한 점을 원점으로부터의 각도와 거리로 표현하는 좌표가 극좌표가 되겠습니다. 이러한 극좌표에 비해서 가장 두드러지는 직교 좌표계의 특징은 임의의 차원으로 확장이 용이하다는 것입니다. 다음은 2차원 공간을 극좌표로 표현한 것입니다.
![polar_coord_2d](/assets/polar_coord_2d.png)
극좌표계는 3차원의 점은 표현할 수 있지만, 그 이상의 차원의 점은 표현하기가 어렵습니다. (적어도 저는 어떻게 해야 할지 잘 모르겠습니다.) 하지만, 직교 좌표계의 경우에는 벡터의 크기를 늘여주기만 하면 보다 높은 차원의 공간을 표시할 수 있습니다.
![polar_coord_3d](/assets/polar_coord_3d.png)


### 2차원 공간부터...

우리가 제일 익숙한 것이 $x$, $y$ 축으로 표현된 직교 좌표계입니다. 이 좌표계에서 가장 먼저 시작해 보죠. 직교 좌표계가 수직선과 수평선으로 이루어져 있다는 것을 알고 있습니다. 이러한 선들을 어떻게 표현할 수 있을까요? 내적 공간은 보통 기저와 원점으로 표현됩니다. 2차원 공간에서 원점은 다음과 같이 나타냅니다.

$$\left(\begin{array}{c}0 \\ 0 \end{array}\right)$$

아무것도 없는 무의 상태에서 원점은 기준점의 역할을 합니다. 
기저는 아무런 벡터로 나타낼 수 있습니다. 내적 공간에는 기본적으로 원소에 어떠한 값을 곱해도 그 공간안에 있어야 하고 원소들끼리 더해도 그 안에 있어야 합니다. 그 원소가 실수이기만 하면 됩니다. 또한 2차원 공간을 생성(span)하기 위해서는 최소한 그 차원만큼의 벡터가 필요합니다.  


2차원의 직교 좌표계는 수직선과 수평선으로 이루어져 있습니다. 수학적으로는 다음과 같은 두개의 벡터의 
$\left(\begin{array}{c}0 \\1\end{array}\right), [1, 0]$이 됩니다. 

2차원 공간을 정의하는 방법은 무수히 많습니다. 원점 0을 지나는 겹치지 않는 어떤 2개의 직선으로도 2차원 공간을 표현할 수 있습니다. 아래의 그림에서 원점을 지나는 두 직선 $a$와 $b$는 그 둘의 선형 결합으로 평면위의 어떠한 점도 표현할 수 있습니다.

![coord_2d_1](/assets/coord_2d_1.png)

우리는 우리는 직교 좌표계에 이미 익숙해져 있습니다. 이러한 직교 좌표계(데카르트 좌표계)에서는 단순히 차원의 갯수만큼 숫자를 쌓아올림으로써 벡터 공간의 원소를 표시합니다.



위의 그림을 해석하면, 붉은 점은 원점으로부터 직선 $a$ 방향으로 $a_0$만큼 직선 $b$ 방향으로 $b_0$만큼 떨어졌다는 것을 의미합니다. 다음과 같이 쓸 수 있습니다.

$$ a_0 \cdot \bm a + b_0 \cdot \bm b$$

위와 같이 말하면 모든 점은 
그림으로 직선 $a$와 $b$는 쉽게 표시할 수 있지만 숫자로는 어떻게 표현할까요? 그렇게 하기 위해서는 어떤 기준이 필요합니다. 그 기준을 표준 기저라고 이야기 합니다. 2차원 평면에서 표준 기저는 다음과 같이 정의합니다.

$\left(\begin{array}{c}0 \\1\end{array}\right), [1, 0]$이 됩니다. 그리고 이렇게 표준기저로 두면 어떠한 직선이든 다음과 같이 표현할 수 있습니다.
$$a \cdot [1,0] + b \cdot [0,1] = [a,b]$$



선형대수에서는 직선을 원점으로부터 시작되는 벡터로 표현할 수 있습니다. 예를 들어, 직선 $a$는 다음과 같이 
직교 좌표계에서는 서로 직각인 2개의 직선으로 표현이 됩니다. 2차원 공간에서의 hyper-plane은 정의에 따라 1차원 직선입니다.  여기서는 Newton-Raphson method의 idea를 얻을 수 있습니다. 
## 3차원 공간에서...

Linear algebra는 3차원 공간에서의 현상을 바탕으로 이해하면 쉽습니다. 그 이상의 고차원은 우리가 상상할 수 없기 때문입니다. 따라서 많은 예제들이 3차원 공간에서 2차원 hyper-plane을 상정합니다. 3차원 공간에서 생각해보도록 하겠습니다.




### 등위 공간, hyper-plane, 법선


## $n$ 차원 공간에서는



## Taylor expansion


